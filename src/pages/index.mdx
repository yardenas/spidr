---
layout: ../layouts/Layout.astro
title: "SPiDR: A Simple Approach for Zero-Shot Safety in Sim-to-Real Transfer"
description: Safe transfer of RL policies from sim to real.
favicon: /spiderman.png
# thumbnail: /screenshot.png
---

import { Image } from "astro:assets";

import Layout from "../layouts/Layout.astro";

import Header from "../components/Header.astro";
import TwoColumns from "../components/TwoColumns.astro";
import Video from "../components/Video.astro";
import YouTubeVideo from "../components/YouTubeVideo.astro";
import PDF from "../components/PDF.astro";
import Figure from "../components/Figure.astro";
import LaTeX from "../components/LaTeX.astro";
import SmallCaps from "../components/SmallCaps.astro";
import Splat from "../components/Splat.tsx"

import CodeBlock from "../components/CodeBlock.astro";
export const components = { pre: CodeBlock }

import spidrIdea from "../assets/spidr-idea.svg";
import demoEnvs from "../assets/demo-envs.png";
import simulatedCosts from "../assets/simulated-costs.svg";
import simulatedAll from "../assets/simulated.svg";

<Header
  title={frontmatter.title}
  authors={[
    {
      name: "Yarden As",
      url: "https://yas.pub",
      institution: "ETH Zurich",
    },
    {
      name: "Chengrui Qu",
      url: "https://crqu.github.io/",
      institution: "Caltech",
    },
    {
      name: "Benjamin Unger",
      institution: "ETH Zurich",
    },
    {
      name: "Dongho Kang",
      url: "https://donghokang.net/",
      institution: "ETH Zurich",
    },
    {
      name: "Max van der Hart",
      institution: "ETH Zurich",
    },
    {
      name: "Laixi Shi",
      url: "https://laixishi.github.io/",
      institution: "John Hopkins University",
    },
    {
      name: "Stelian Coros",
      institution: "ETH Zurich",
      url: "https://crl.ethz.ch/people/coros/"
    },
    {
      name: "Adam Wierman",
      institution: "Caltech",
      url: "https://adamwierman.com/"
    },
    {
      name: "Andreas Krause",
      url: "https://las.inf.ethz.ch/krausea",
      institution: "ETH Zurich",
    },
  ]}
  conference="NeurIPS 2025"
  notes={[
  ]}
  links={[
    {
      name: "Code",
      url: "https://github.com/yardenas/safe-learning",
      icon: "mdi:github",
    },
    {
      name: "arXiv",
      url: "https://arxiv.org/abs/2509.18648",
      icon: "academicons:arxiv",
    },{
      name: "Paper",
      url: "https://openreview.net/forum?id=Pe1ypX9gBO",
      icon: "ri:file-pdf-2-line",
    },
  ]}
  />
  

<div class="container mx-auto py-10">
  <div class="flex flex-col lg:flex-row justify-center lg:space-x-4 space-y-4 lg:space-y-0">
    <Image src={spidrIdea} alt="Core idea of SPiDR" class="object-cover w-full h-full" />
  </div>
</div>


We introduce SPiDR (**S**im-to-real via **P**ess**i**mistic **D**omain **R**andomization), a scalable algorithm that closes the gap between simulation and reality without compromising safety. SPiDR uses domain randomization to incorporate the uncertainty about the sim-to-real gap into the safety constraints, making it versatile and highly compatible with existing training pipelines.

We theoretically show that unsafe transfer can be related to large uncertainty about the sim-to-real gap, quantified as the disagreement among next-state predictions from domain-randomized dynamics models. This key idea is illustrated in the figure above, where spikes in uncertainty (e.g. at t = 4.6 and t = 5.3) coincide with unstable or unsafe behaviors, such as stumbling or flipping. Motivated by this insight, we propose to penalize the cost with the uncertainty to achieve safe sim-to-real transfer, leading to the design of SPiDR.


## Domain Randomization is Not Safe
Domain randomization is the de facto method for reliable transfer for simulator to real robots. 
Safety is a key component in robotics, however it is not addressed directly by domain randomization. This means that you can train your robot in simulation, and it may still violate safety constraints (like falling or overusing the motors) when you get to deploy the policy on the real system. Existing methods for safe transfer typically rely on tools from robust optimization, therefore requiring roboticists to significantly alter our beloved domain randomization training pipelines. We started this project with the following question:

> How can we develop a safe sim-to-real reinforcement learning algorithm that practitioners can use without reinventing the wheel?

With that in mind, the first obvious thing to do is check is just using domain randomization can still satisfy safety constraints, even when under distribution shifts. The figure below shows that using domain randomization fails to satisfy safety constraints on a bunch of well-known tasks in Mujoco. In contrast, when using SPiDR, constraints are satisfied across all tasks.
<Figure caption="Domain randomization does not satsify the safety constraints.">
  <Image src={simulatedCosts} alt="Domain randomization does not satisfy safety constraints under mismatches in the dynamics." class="object-cover w-full h-full"/>
</Figure>

## What About Performance?
We saw that by adding pessimis, SPiDR is able to transfer safely. But what if with this added conservatism SPiDR is just safe, but does not really solve the task? In the figure below we show the performance in the y-axis vs. safety in the x-axis (upper-left is better). As shown, among all baselines, SPiDR consistently satisfies the constraints while achieving good performance.
<Figure caption="SPiDR finds good balance between safety and performance">
  <Image src={simulatedAll} alt="SPiDR finds good balance between safety and performance." class="object-cover w-full h-full"/>
</Figure>




We evaluate ActSafe on the Pendulum environment.
We visualize the trajectories of ActSafe and its unsafe variant in the state space during exploration. 
We observe that both algorithms cover the state space well, however, ActSafe remains within the safety boundary during learning whereas its unsafe version violates the constraints.
{/* <Figure caption="Safe exploration in the PendulumSwingup task.
Each plot above visualizes trajectories considered during exploration across all past learning episodes.
The red box in the plot depicts the safety boundary in the state space. ActSafe maintains safety throughout learning.">
<Image src={pendulum} alt="Pendulum safe exploration" class="object-cover w-full h-full" />
</Figure> */}


### Cartpole

We evaluate on CartpoleSwingupSparse from the RWRL benchmark, where the goal is to swing up the pendulum, while keeping the cart at the center.
We add penalty for large actions to make exploration even more challenging.
We compare ActSafe with three baselines:
* Uniform, which samples actions uniformly at random during exploration.
* Optimistic, which uses the model epistemic uncertainty estimates as exploration reward bonuses.
* Greedy, which optimizes the extrinsic reward directly.

{/* <Figure caption="Hard exploration on Cartpole.">
<Image width="500" src={cartpoleLearning} alt="Cartpole Exploration"/>
</Figure> */}

#### Sparse-reward Navigation
In this experiment, we examine the influence of using an intrinsic reward in hard exploration tasks.
To this end, we extend tasks from SafetyGym and introduce three new tasks with sparse rewards, i.e., without any reward shaping to guide the agent to the goal.
We provide more details about the rewards in the figure below. In the figure below we compare ActSafe with a Greedy baseline that collects trajectories only based on the sparse extrinsic reward.
As shown, ActSafe substantially outperforms Greedy in all tasks, while violating the constraint only once in the GotoGoal task.

{/* <Figure caption="Hard exploration in navigation tasks.">
<Image src={safetyGym} alt="Expansion Process" class="w-full"/>
</Figure> */}

## Cite

```
@inproceedings{
  as2025spidrsimpleapproachzeroshot,
  title={{SP}i{DR}: A Simple Approach for Zero-Shot Safety in Sim-to-Real Transfer},
  author={Yarden As and Chengrui Qu and Benjamin Unger and Dongho Kang and Max van der Hart and Laixi Shi and Stelian Coros and Adam Wierman and Andreas Krause},
  booktitle={International Conference on Neural Information Processing Systems},
  year={2025},
}
```

